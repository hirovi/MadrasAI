{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10 \n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=cwd + '/data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=cwd + '/data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize example input image\n",
    "train_iter = iter(train_loader)\n",
    "images, labels = train_iter.next()\n",
    "\n",
    "img = images[0]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img.data.numpy().reshape((28,28)), cmap = plt.cm.gray)\n",
    "print('Input image')\n",
    "print(img.shape) # 1 channel, WxH = 28x28\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (2 convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        Convolution layer 1\n",
    "        Input: 1x28x28\n",
    "        Output: 16x14x14\n",
    "        '''\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        '''\n",
    "        Convolution layer 2\n",
    "        Input: 16x14x14\n",
    "        Output: 32x7x7\n",
    "        '''\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fc = nn.Linear(32*7*7, num_classes)\n",
    "        \n",
    "        #self.softmax = nn.Softmax()\n",
    "        \n",
    "    def Flatten(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.Flatten(out)\n",
    "        out = self.fc(out)\n",
    "        #out = self.softmax(out) ####SOFTMAX IS INCLUDED IN CrossEntropyLoss in Pytorch\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate net\n",
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.4423\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1144\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0931\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1329\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1058\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0854\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1881\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0446\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0164\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0710\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1115\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0420\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0765\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0280\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0706\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0488\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0169\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0897\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0392\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0111\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0186\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0516\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0916\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0123\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0288\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0029\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0466\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0074\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0053\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "## Trainining Pipeline\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss() #Includes softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward Pass\n",
    "        optimizer.zero_grad() # Clear stored gradients\n",
    "        outputs = model(images) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate error\n",
    "        \n",
    "        #Backward and optimize\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFilename = 'mnist-cnn.model'\n",
    "\n",
    "# Save the Trained Model\n",
    "if not os.path.exists(modelFilename):\n",
    "    torch.save(model.state_dict(), modelFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.94 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
