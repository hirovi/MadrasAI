{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10 \n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=cwd + '/data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=cwd + '/data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize example input image\n",
    "train_iter = iter(train_loader)\n",
    "images, labels = train_iter.next()\n",
    "\n",
    "img = images[0]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img.data.numpy().reshape((28,28)), cmap = plt.cm.gray)\n",
    "print('Input image')\n",
    "print(img.shape) # 1 channel, WxH = 28x28\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (2 convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        Convolution layer 1\n",
    "        Input: 1x28x28\n",
    "        Output: 16x14x14\n",
    "        '''\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        '''\n",
    "        Convolution layer 2\n",
    "        Input: 16x14x14\n",
    "        Output: 32x7x7\n",
    "        '''\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fc = nn.Linear(32*7*7, num_classes)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def Flatten(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.Flatten(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate net\n",
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hirovi/miniconda3/envs/deeplearning/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 1.7623\n",
      "Epoch [1/5], Step [200/600], Loss: 1.7145\n",
      "Epoch [1/5], Step [300/600], Loss: 1.5627\n",
      "Epoch [1/5], Step [400/600], Loss: 1.6158\n",
      "Epoch [1/5], Step [500/600], Loss: 1.5971\n",
      "Epoch [1/5], Step [600/600], Loss: 1.5692\n",
      "Epoch [2/5], Step [100/600], Loss: 1.5910\n",
      "Epoch [2/5], Step [200/600], Loss: 1.5674\n",
      "Epoch [2/5], Step [300/600], Loss: 1.5432\n",
      "Epoch [2/5], Step [400/600], Loss: 1.5671\n",
      "Epoch [2/5], Step [500/600], Loss: 1.5707\n",
      "Epoch [2/5], Step [600/600], Loss: 1.5698\n",
      "Epoch [3/5], Step [100/600], Loss: 1.6161\n",
      "Epoch [3/5], Step [200/600], Loss: 1.5202\n",
      "Epoch [3/5], Step [300/600], Loss: 1.4925\n",
      "Epoch [3/5], Step [400/600], Loss: 1.5364\n",
      "Epoch [3/5], Step [500/600], Loss: 1.5505\n",
      "Epoch [3/5], Step [600/600], Loss: 1.5085\n",
      "Epoch [4/5], Step [100/600], Loss: 1.5025\n",
      "Epoch [4/5], Step [200/600], Loss: 1.4629\n",
      "Epoch [4/5], Step [300/600], Loss: 1.4993\n",
      "Epoch [4/5], Step [400/600], Loss: 1.4796\n",
      "Epoch [4/5], Step [500/600], Loss: 1.4885\n",
      "Epoch [4/5], Step [600/600], Loss: 1.4917\n",
      "Epoch [5/5], Step [100/600], Loss: 1.4760\n",
      "Epoch [5/5], Step [200/600], Loss: 1.4755\n",
      "Epoch [5/5], Step [300/600], Loss: 1.4696\n",
      "Epoch [5/5], Step [400/600], Loss: 1.4614\n",
      "Epoch [5/5], Step [500/600], Loss: 1.4704\n",
      "Epoch [5/5], Step [600/600], Loss: 1.4927\n"
     ]
    }
   ],
   "source": [
    "## Trainining Pipeline\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward Pass\n",
    "        optimizer.zero_grad() # Clear stored gradients\n",
    "        outputs = model(images) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate error\n",
    "        \n",
    "        #Backward and optimize\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFilename = 'mnist-cnn.model'\n",
    "\n",
    "# Save the Trained Model\n",
    "if not os.path.exists(modelFilename):\n",
    "    torch.save(model.state_dict(), modelFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
