{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd Tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches for Computing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Symbolic differentiation:** automatic manipulation of mathematical expressions to get derivatives\n",
    "    - Takes a math expression and returns a math expression: $f(x) = x^2 \\rightarrow \\frac{df(x)}{dx} = 2x$\n",
    "    - Used in Mathematica, Maple, Sympy, etc.\n",
    "* **Numeric differentiation:** Approximating derivatives by finite differences:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i} f(x_1, \\dots, x_N) = \\lim_{h \\to 0} \\frac{f(x_1, \\dots, x_i + h, \\dots, x_N) - f(x_1, \\dots, x_i - h, \\dots, x_N)}{2h}\n",
    "$$\n",
    "* **Automatic differentiation:** Takes code that computes a function and returns code that computes the derivative of that function.\n",
    "    - Reverse Mode AD: A method to get exact derivatives efficiently, by storing information as you go forward that you can reuse as you go backwards\n",
    "    - \"The goal isn't to obtain closed-form solutions, but to be able to wirte a program that efficiently computes the derivatives.\" - Lecture 6 Slides (Backpropagation)\n",
    "    - **Autograd**, **Torch Autograd**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "* [Autograd](https://github.com/HIPS/autograd) is a Python package for automatic differentiation.\n",
    "* To install Autograd:\n",
    "                pip install autograd\n",
    "* There are a lot of great [examples](https://github.com/HIPS/autograd/tree/master/examples) provided with the source code\n",
    "\n",
    "### What can Autograd do?\n",
    "\n",
    "From the Autograd Github repository:\n",
    "\n",
    "* Autograd can automatically differentiate native Python and Numpy code.\n",
    "* It can handle a large subset of Python's features, including loops, conditional statements (if/else), recursion and closures\n",
    "* It can also compute higher-order derivatives\n",
    "* It uses reverse-mode differentiation (a.k.a. backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments.\n",
    "\n",
    "\n",
    "## Autograd vs Tensorflow, Theano, etc.\n",
    "\n",
    "Many deep learning packages implement automatic differentiation using small _domain-specific languages_ within Python:\n",
    "    - Theano\n",
    "    - Caffe\n",
    "    - Vanilla Torch (as compared to Autograd for Torch)\n",
    "    - Tensorflow\n",
    "Most of these alternatives require you to _explicitly_ construct a computation graph; Autograd constructs a computation graph _implicitly_, by tracking the sequence of operations that have been performed during the execution of a program.\n",
    "\n",
    "## GPU Support for Autograd?\n",
    "\n",
    "There are a couple projects to look into if you are interested in GPU support for Autograd:\n",
    "\n",
    "* [MinPy](https://github.com/dmlc/minpy)\n",
    "* [PyTorch](http://pytorch.org/)\n",
    "\n",
    "## Autograd Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thinly-wrapped numpy\n",
    "import autograd.numpy as np\n",
    "# Basicallly the only function you need\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39322386648296376\n",
      "0.39322386636453377\n"
     ]
    }
   ],
   "source": [
    "# Define a normal function, using python & numpy\n",
    "def tanh(x):\n",
    "    y = np.exp(-x)\n",
    "    return (1.-y)/(1.+y)\n",
    "\n",
    "grad_tanh = grad(tanh)\n",
    "\n",
    "# Evaluate the gradient at x=10\n",
    "print(grad_tanh(1.0))\n",
    "\n",
    "# Compare to numeric gradient computed using finite differences\n",
    "print((tanh(1.0001) - tanh(0.9999)) / 0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd vs Manual Gradients via Staged Computation\n",
    "\n",
    "In this example, we will see how a complicated computation can be written as a composition of simpler functions, and how this provides a scalable strategy for computing gradients using the chain rule.\n",
    "\n",
    "Say we want to write a function to compute the gradient of the *sigmoid function*:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "We can write $\\sigma(x)$ as a composition of several elementary functions, as $\\sigma(x) = s(c(b(a(x))))$, where:\n",
    "\n",
    "$$\n",
    "a(x) = -x\n",
    "$$\n",
    "\n",
    "$$\n",
    "b(a) = e^a\n",
    "$$\n",
    "\n",
    "$$\n",
    "c(b) = 1 + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "s(c) = \\frac{1}{c}\n",
    "$$\n",
    "\n",
    "Here, we have \"staged\" the computation such that it contains several intermediate variables, each of which are basic expressions for which we can easily compute the local gradients.\n",
    "\n",
    "The computation graph for this expression is shown in the figure below. \n",
    "\n",
    "![Gradient Computation Image](images/ComputationGraph.png)\n",
    "\n",
    "The input to this function is $x$, and the output is represented by node $s$. We wish compute the gradient of $s$ with respect to $x$, $\\frac{\\partial s}{\\partial x}$. In order to make use of our intermediate computations, we can use the chain rule as follows:\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x}\n",
    "$$\n",
    "\n",
    "<!--\n",
    "Given a vector-to-scalar function, $\\mathbb{R}^D \\to \\mathbb{R}$, composed of a set of primitive functions\n",
    "$\\mathbb{R}^M \\to \\mathbb{R}^N$ (for various $M$, $N$), the gradient of the composition is given by the product of the gradients of the primitive functions, according to the chain rule. But the chain rule doesnâ€™t prescribe the order in which to multiply the gradients. From the perspective of computational complexity, the order makes all the\n",
    "difference.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual calculation of gradient:    0.1049935854035065\n",
      "Automatic calculation of gradient: 0.1049935854035065\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1.0 / (1.0 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def grad_sigmoid_manual(x):\n",
    "    '''\n",
    "    Define the gradient of the logistic sigmoid function\n",
    "    '''\n",
    "    # First define the elementary functions of the sigmoid function as seen above.\n",
    "    a = -x         \n",
    "    b = np.exp(a)  \n",
    "    c = 1 + b      \n",
    "    s = 1.0 / c \n",
    "    \n",
    "    # Backward pass\n",
    "    dsdc = (-1.0 / (c**2))\n",
    "    dsdb = dsdc * 1\n",
    "    dsda = dsdb * np.exp(a)\n",
    "    dsdx = dsda * (-1)\n",
    "    \n",
    "    return dsdx\n",
    "\n",
    "## Manually without Autograd\n",
    "print('Manual calculation of gradient:   ',grad_sigmoid_manual(2.0))\n",
    "\n",
    "## Automatically with Autograd\n",
    "grad_sigmoid_automatic = grad(sigmoid)\n",
    "print('Automatic calculation of gradient:',grad_sigmoid_automatic(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know what you're thinking... Autograd is amazing!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients of Data Structures: flatten and unflatten\n",
    "                                                                     \n",
    "Autograd allows you to compute gradients for many different data structures. Autograd provides a lot of flexibility in the types of data structures you can use to store the parameters of your model. This flexibility is achieved through the flatten function, which converts any nested combination of lists, tuples, arrays, or dicts into a 1-dimensional Numpy array.                                              \n",
    "                                                                                                      \n",
    "The idea is that we know how to compute gradients of vectors, and we can convert many data structures into vectors (i.e. \"flatten\" the data structures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.misc import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened: [1. 1. 2. 2. 3. 3. 3.]\n",
      "Unflattened: [(array(1.), array(1.)), (array(2.), array(2.)), (array(3.), array(3.), array(3.))]\n"
     ]
    }
   ],
   "source": [
    "# You can flatten a list of tupples\n",
    "params = [(1.0, 1.0), (2.0,2.0), (3.0,3.0,3.0)]\n",
    "flat_params, unflatten_func = flatten(params)\n",
    "print('Flattened: {}'.format(flat_params))\n",
    "print('Unflattened: {}'.format(unflatten_func(flat_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
